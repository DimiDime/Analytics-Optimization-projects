{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![uc3m](img/uc3m.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Stochastic Gradient Descent and Least Squares\n",
    "\n",
    "<a href=\"http://www.est.uc3m.es/nogales\" target=\"_blank\">Javier Nogales</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you rewrite model for lin regression you get additional 0.25 Poinits\n",
    "- Change with any ML model\n",
    "- Until 11.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The general framework in machine learning and statistics is:\n",
    "\n",
    "Data = Model + Noise\n",
    "\n",
    "In Statistics, the Model is usually known up to some parameters, whereas in Machine Learning the Model is learnt through the data.\n",
    "\n",
    "Consider the following linear model and data:\n",
    "$$y=\\beta_{0}+\\beta_{1}x_1+\\dots+\\beta_{p}x_p+e$$\n",
    "where $e\\sim N(0,\\sigma^{2})$\n",
    "\n",
    "We will learn how to apply the Stochastic Gradient method to the least-squares problem\n",
    "\n",
    "<img src=\"img/SG.png\" width=\"450\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random data from previous model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "\n",
    "nsample = 1000 # Observations\n",
    "nvariables=10 # Samples\n",
    "\n",
    "# Following the formula\n",
    "X0 = np.ones([nsample,1]) #the firt column are ones for the beta_0\n",
    "X1 = np.random.uniform(0,10,([nsample,nvariables])) # random numbs 0 - 10\n",
    "X = np.concatenate([X0, X1],axis=1)\n",
    "\n",
    "beta=np.random.randint(-10,10,size=([nvariables+1,1])) # random numbs -10 - 10\n",
    "error=np.random.normal(0,6,(nsample,1)) #normal random error\n",
    "\n",
    "Y=np.dot(X,beta)+error\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a linear relation between a set of variables ($X$) with respect to a response variable ($y$)\n",
    "\n",
    "Model: $y = X\\beta + u$\n",
    "\n",
    "Classical estimation: least squares\n",
    "\n",
    "  \\begin{align*}\n",
    "\\text{minimize}\\quad & \\frac{1}{n}||y-X\\beta||_2^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LS solution using textbook formula is: $\\beta_{ls}=(X^T X)^{-1}X^T y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.66005715]\n",
      " [ 6.96292181]\n",
      " [-0.02783633]\n",
      " [ 6.07039611]\n",
      " [-1.06777975]\n",
      " [ 2.99381691]\n",
      " [-0.02835074]\n",
      " [-5.96335833]\n",
      " [-2.95418012]\n",
      " [-6.98044486]\n",
      " [-0.12237511]]\n"
     ]
    }
   ],
   "source": [
    "beta_ls_exact=np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(X),X)),np.transpose(X)),Y)\n",
    "\n",
    "print(beta_ls_exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with the true solution $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8]\n",
      " [ 7]\n",
      " [ 0]\n",
      " [ 6]\n",
      " [-1]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [-6]\n",
      " [-3]\n",
      " [-7]\n",
      " [ 0]]\n"
     ]
    }
   ],
   "source": [
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Stochastic Gradient\n",
    "\n",
    "The loss function is $l(w)=\\frac{1}{n}||y-X\\beta||_2^2=\\frac{1}{n}\\sum_{i=1}^n(y_i-\\beta' x_i)^2$, hence the gradient is\n",
    "\n",
    "$$\n",
    "  g_k = \\nabla l(w) = -\\frac{1}{n}2X'(y-X\\beta)\n",
    "$$\n",
    "\n",
    "This is an expensive gradient because it is using all the information in $(X,y)$\n",
    "\n",
    "The Stochastic Gradient will use only one random sample ($i$) at each iteration, hence $\\hat{g}_k = 2 x_i(y_i-x_i'\\beta)$, so that starting at $\\beta_0$, we compute $\\beta_{k+1}$ as \n",
    "\n",
    "$$\\beta_{k+1}=\\beta_k-\\alpha \\hat{g}_k$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.54237796  6.99622862 -0.01694278  5.46794307 -0.9557095   2.89029924\n",
      " -0.36252295 -6.23839327 -2.77163606 -6.86774362 -0.35740443]\n",
      "iterations: 200000\n",
      "error 0.05117128465167299\n"
     ]
    }
   ],
   "source": [
    "(n, p) = X.shape\n",
    "# one epoch = 1000 iterations\n",
    "niter = 200*n # equivalent to 200 epoch \n",
    "k = 0\n",
    "alpha = 0.001 # learning rate (hyper-parameter)\n",
    "beta_stoch = np.zeros(p)\n",
    "\n",
    "while (k < niter):\n",
    "    \n",
    "    i = random.choice(range(n)) # one sample by chance\n",
    "    stoch_grad = -2*X[i,]*(Y[i]-np.dot(beta_stoch,X[i,])) # compute SGD\n",
    "    beta_stoch = beta_stoch-alpha*stoch_grad # the movement\n",
    "    # Most important lines in DL\n",
    "    k +=1\n",
    "    \n",
    "print(beta_stoch)\n",
    "print('iterations:',k)\n",
    "print('error', np.linalg.norm(np.transpose(beta_ls_exact) - beta_stoch)/np.linalg.norm(beta_ls_exact)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Mini-Batch SG\n",
    "- Using a small sample\n",
    "\n",
    "Loss = -2 * Xt * (y - X * beta )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.94684906e+00  7.06260470e+00  6.42263303e-03  6.12142080e+00\n",
      " -1.07563110e+00  3.02509750e+00  6.96097543e-03 -5.91618612e+00\n",
      " -2.93315331e+00 -6.97238233e+00 -1.34539725e-01]\n",
      "iterations = 25001\n",
      "error= 0.10588688686205568\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "### Redefine gradient function, to calculate the gradient just with the indexes passed as parameters\n",
    "def least_sq_reg_der_stoc(beta_ls,X,Y,subset,B):\n",
    "    beta_ls=np.matrix(beta_ls)\n",
    "    Xsub = np.matrix(X[subset,:])\n",
    "    Ysub = np.matrix(Y[subset])    \n",
    "    pp=-2*np.dot((Ysub-np.dot(Xsub,beta_ls.T)).T,Xsub).T\n",
    "    aa= np.squeeze(np.asarray(pp))/B\n",
    "    return aa\n",
    "\n",
    "(n,p)=X.shape\n",
    "\n",
    "B = 20 # Batch size (hyper-parameter)\n",
    "niter = 500*n/B # equivalent to 500 epoch\n",
    "alpha=0.001 # learning rate (hyper-parameter)\n",
    "beta_lsg=np.zeros(p) #initial value for beta\n",
    "\n",
    "k=0\n",
    "\n",
    "while (k <= niter):\n",
    "    # Same alg just changed \n",
    "    subset = np.random.choice([x for x in range(0,n)],B)\n",
    "    grad = least_sq_reg_der_stoc(beta_lsg, X, Y,subset,B)\n",
    "    beta_lsg = beta_lsg - alpha * grad\n",
    "\n",
    "    k +=1\n",
    "\n",
    "print(beta_lsg)\n",
    "print('iterations =',k)\n",
    "print('error=',np.linalg.norm(np.transpose(beta_ls_exact)-beta_lsg)/np.linalg.norm(beta_ls_exact))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.32867826e+00  7.02310657e+00  5.73771144e-02  6.00864264e+00\n",
      " -1.06925269e+00  3.03903524e+00  4.78202967e-03 -5.79081422e+00\n",
      " -3.06299864e+00 -7.02836938e+00 -1.48720795e-01]\n",
      "iterations = 20001\n",
      "error= 0.025569876382360912\n"
     ]
    }
   ],
   "source": [
    "(n,p)=X.shape\n",
    "\n",
    "B = 10 # Batch size (hyper-parameter)\n",
    "niter = 200*n/B # equivalent to 200 epoch\n",
    "alpha=0.001 # learning rate (hyper-parameter)\n",
    "beta_mom=np.zeros(p) #initial value for beta\n",
    "\n",
    "v=np.zeros(p) # momentum vector\n",
    "\n",
    "# Weight to give to the past parameter (60% weight(importance)) \n",
    "nu = 0.6 # momentum rate (hyper-parameter)\n",
    "\n",
    "k=0\n",
    "\n",
    "while (k <= niter):\n",
    "    \n",
    "    subset = np.random.choice([x for x in range(0,n)],B)\n",
    "    grad = least_sq_reg_der_stoc(beta_mom, X, Y,subset,B)\n",
    "    # Extra line for the !!momentum!!\n",
    "    # Information from past iterations (Memory)\n",
    "    # Improve speed of the alg\n",
    "    v = grad + nu*v  # current gradient + previous direction == Next direction is closer to the next next direction\n",
    "\n",
    "    beta_mom = beta_mom - alpha * v\n",
    "\n",
    "    k +=1\n",
    "\n",
    "print(beta_mom)\n",
    "print('iterations =',k)\n",
    "print('error=',np.linalg.norm(np.transpose(beta_ls_exact)-beta_mom)/np.linalg.norm(beta_ls_exact))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Apply the same algorithms to a logistic regression problem, or other machine learning tool that can be defined using a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
