{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![uc3m](img/uc3m.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms for Unconstrained Optimization\n",
    "\n",
    "<a href=\"http://www.est.uc3m.es/nogales\" target=\"_blank\">Javier Nogales</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in minimizing $f(x)$ where $x$ is a vector\n",
    "\n",
    "![s](img/grad.png)\n",
    "\n",
    "Algorithms are based on iterations: $x_{k+1}=x_k + \\alpha_k p_k$\n",
    "\n",
    "Ideally, algorithms should satisfy: $f(x_{k+1})<f(x_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General framework for algorithms\n",
    "\n",
    "![s](img/algo.png)\n",
    "\n",
    "Many versions depending on the choice of the search direction and step length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gradient Algorithm\n",
    "\n",
    "Simple idea with $p_k=-\\nabla f(x_k)$\n",
    "\n",
    "We will implement the algorithms with automatic differenciation (for the gradients) using Tensorflow\n",
    "    \n",
    "    conda install -c conda-forge tensorflow \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TensorFlow is an open source software library for numerical computations written in C++ (not confined to NNs)\n",
    "\n",
    "- Originally developed by researchers in Google to perform distributed computation, deal with large datasets, automatic differentiation, optimization algorithms, etc.\n",
    "\n",
    "The automatic differenciation of TensorFlow is based on the GradientTape API \n",
    "\n",
    "The next function provides the gradient and hessian matrix for any function $f$ at any point $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivatives(x, f, order = 1):\n",
    "    \n",
    "    x = tf.Variable(x)\n",
    "\n",
    "    with tf.GradientTape() as t1:\n",
    "        with tf.GradientTape() as t2:\n",
    "            y = f(x)\n",
    "            \n",
    "        g = t2.gradient(y, x) # the gradient vector\n",
    "    \n",
    "    if order == 2:\n",
    "        g = [g, t1.jacobian(g,x)] # the hessian matrix\n",
    "        \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to implement the Gradient Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gradient algorithm\n",
    "\n",
    "def gradient(fun, x0, sigma = 0.001, beta = 0.5, verbose = 0, itmax = 100):\n",
    "    \n",
    "    f = fun(x0)\n",
    "    g = derivatives(x0, fun).numpy()\n",
    "    it = 1; x = x0; \n",
    "    \n",
    "    while (np.linalg.norm(g) > 1.0e-6) & (it < itmax):\n",
    "\n",
    "        g = derivatives(x, fun).numpy() # the gradient\n",
    "        p = -g # the search direction; divide by ||g|| to scale better, not mandatory\n",
    "\n",
    "        alpha = 1 # the step length\n",
    "        ff = fun(x + alpha * p)\n",
    "        # backtracking\n",
    "        it_back = 1\n",
    "        while (ff > f + sigma * alpha * np.dot(p, g)) & (it_back<20): # Armijo rule           \n",
    "            alpha = alpha * beta\n",
    "            ff = fun(x + alpha * p)\n",
    "            it_back = it_back+1\n",
    "        \n",
    "        # Movement and updates\n",
    "        x = x + alpha * p\n",
    "        f = ff \n",
    "        g = derivatives(x, fun).numpy()\n",
    "        it = it + 1\n",
    "        \n",
    "    # Output format\n",
    "    if it == itmax:\n",
    "        print('The algorithm does not converge for {:d} iterations'.format(itmax)) \n",
    "        \n",
    "    if verbose > 0 and it < itmax:\n",
    "        print('The algorithm converges in {:d} iterations'.format(it))\n",
    "        \n",
    "    if verbose == 1 or verbose == 2:\n",
    "        print('The final point is ({:.3}, {:.3})'.format(x[0],x[1]))\n",
    "            \n",
    "        if verbose == 2:\n",
    "            print('The optimal value function is {:.3} \\n'.format(ff))\n",
    "            \n",
    "    return x, ff, it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four examples\n",
    "\n",
    "![s](img/FourExamples.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = lambda x: x[0]**2 + 5 * x[1]**2 + x[0] - 5 * x[1]\n",
    "f2 = lambda x: x[0]**2 + 5 * x[0] * x[1] + 100 * x[1]**2 - x[0] + 4 * x[1]\n",
    "f3 = lambda x: np.e**(x[0] + 3 * x[1] - 0.1) + np.e**(x[0] - 3 * x[1] -0.1) + np.e**(-x[0] - 0.1)\n",
    "f4 = lambda x: (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The algorithm converges in 14 iterations\n",
      "The final point is (-0.5, 0.5)\n",
      "The optimal value function is -1.5 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "x0=np.random.rand(2)*10\n",
    "x, ff, it = gradient(f1, x0, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The algorithm does not converge for 100 iterations\n",
      "The final point is (1.07, -0.0568)\n",
      "The optimal value function is -0.136 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "x0=np.random.rand(2)*10\n",
    "x, ff, it = gradient(f2, x0, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a difficult function, let's change some hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The algorithm does not converge for 200 iterations\n",
      "The final point is (0.576, -0.0342)\n",
      "The optimal value function is -0.363 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, ff, it = gradient(f2, x0, sigma=0.5, beta=0.5, itmax=200, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still difficult. Let's try $f_3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/javiernogales/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in double_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The algorithm converges in 46 iterations\n",
      "The final point is (-0.347, -3.85e-08)\n",
      "The optimal value function is 2.56 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "x0=np.random.rand(2)*10\n",
    "x, ff, it = gradient(f3, x0, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the Rosenbrock's function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The algorithm does not converge for 100 iterations\n",
      "The final point is (0.934, 0.873)\n",
      "The optimal value function is 0.00434 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([-1.2,1.0]) # difficult initial point\n",
    "x, ff, it = gradient(f4, x0, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change some hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The algorithm does not converge for 400 iterations\n",
      "The final point is (0.867, 0.75)\n",
      "The optimal value function is 0.0179 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, ff, it = gradient(f4, x0, sigma = 0.1, beta = 0.5, itmax=400, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function requires second-order information to converge better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(fun, x0, sigma = 0.001, beta = 0.5, verbose = 0, itmax = 100):\n",
    "    \n",
    "    f = fun(x0)\n",
    "    g = derivatives(x0, fun).numpy()\n",
    "    it = 1; x = x0; \n",
    "    \n",
    "    while (np.linalg.norm(g) > 1.0e-6) & (it < itmax):\n",
    "        \n",
    "        g, H = derivatives(x, fun, 2) # the gradient and hessian\n",
    "        g = g.numpy() \n",
    "        H = H.numpy()\n",
    "\n",
    "        p = - np.dot(np.linalg.inv(H), g) # the search direction\n",
    "\n",
    "        alpha = 1 # the step length\n",
    "        ff = fun(x + alpha * p)\n",
    "        # backtracking\n",
    "        it_back = 1\n",
    "        while (ff > f + sigma * alpha * np.dot(p, g)) & (it_back<20): # Armijo rule           \n",
    "            alpha = alpha * beta\n",
    "            ff = fun(x + alpha * p)\n",
    "            it_back = it_back+1\n",
    "        \n",
    "        # Movement and updates\n",
    "        x = x + alpha * p\n",
    "        f = ff \n",
    "        g = derivatives(x, fun).numpy()\n",
    "        it = it + 1\n",
    "        \n",
    "    # Output format\n",
    "    if it == itmax:\n",
    "        print('The algorithm does not converge for {:d} iterations'.format(itmax)) \n",
    "        \n",
    "    if verbose > 0 and it < itmax:\n",
    "        print('The algorithm converges in {:d} iterations'.format(it))\n",
    "        \n",
    "    if verbose == 1 or verbose == 2:\n",
    "        print('The final point is ({:.3}, {:.3})'.format(x[0],x[1]))\n",
    "            \n",
    "        if verbose == 2:\n",
    "            print('The optimal value function is {:.3} \\n'.format(ff))\n",
    "            \n",
    "    return x, ff, it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take care: we need to start close to the solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The algorithm converges in 2 iterations\n",
      "The final point is (0.587, -0.0347)\n",
      "The optimal value function is -0.363 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "x0=np.random.rand(2)*10\n",
    "x, ff, it = newton(f2, x0, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The algorithm converges in 22 iterations\n",
      "The final point is (1.0, 1.0)\n",
      "The optimal value function is 3.74e-21 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([-1.2,1.0]) # difficult initial point\n",
    "x, ff, it = newton(f4, x0, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to assure Newton's method converges far away from the solution?\n",
    "\n",
    "We need to modify the Hessian matrix to make it positive definite at any point\n",
    "\n",
    "### Modified Newton's method"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# insert this code above just before definition of p    \n",
    "    e = min(np.linalg.eigvalsh(H))\n",
    "    if e < 0:\n",
    "        H = H + (-e + 0.01) * np.eye(len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The quasi-Newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qnewton(fun, x0, sigma = 0.001, beta = 0.5, verbose = 0, itmax = 100):\n",
    "    \n",
    "    f = fun(x0)\n",
    "    g = derivatives(x0, fun).numpy()\n",
    "    it = 1; x = x0; x_prev = 0; g_prev = 0\n",
    "    \n",
    "    while (np.linalg.norm(g) > 1.0e-6) & (it < itmax):\n",
    "                \n",
    "        g = derivatives(x, fun).numpy() # the gradient, no need of hessian\n",
    "    \n",
    "        # quasi-Newton search direction\n",
    "        if it == 1:\n",
    "            # Identity matrix in first iteration\n",
    "            B = np.eye(len(x))\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            # BFGS update\n",
    "            s = x - x_prev \n",
    "            y = g - g_prev\n",
    "            Bs = np.dot(B_prev, s) \n",
    "            sBs = np.dot(s, Bs)\n",
    "            ys = np.dot(y, s)\n",
    "                \n",
    "            B = B_prev - sBs**-1 * np.dot(Bs.reshape(1, len(Bs)).transpose(),\n",
    "                                 Bs.reshape(1, len(Bs))) + ys **-1 * np.dot(y.reshape(1, len(y)).transpose(),\n",
    "                                                                            y.reshape(1, len(y)))\n",
    "        \n",
    "        p = - np.dot(np.linalg.inv(B), g) # the search direction     \n",
    "        B_prev = B\n",
    "        \n",
    "        # end of quasi-Newton\n",
    "        \n",
    "        alpha = 1 # the step length\n",
    "        ff = fun(x + alpha * p)\n",
    "        # backtracking\n",
    "        it_back = 1\n",
    "        while (ff > f + sigma * alpha * np.dot(p, g)) & (it_back<20): # Armijo rule           \n",
    "            alpha = alpha * beta\n",
    "            ff = fun(x + alpha * p)\n",
    "            it_back = it_back+1\n",
    "        \n",
    "        # Movement and updates\n",
    "        x_prev = np.copy(x); x = x + alpha * p\n",
    "        f = ff \n",
    "        g_prev = np.copy(g); g = derivatives(x, fun).numpy()\n",
    "        it = it + 1\n",
    "        \n",
    " \n",
    "    # Output format\n",
    "    if it == itmax:\n",
    "        print('The algorithm does not converge for {:d} iterations'.format(itmax)) \n",
    "        \n",
    "    if verbose > 0 and it < itmax:\n",
    "        print('The algorithm converges in {:d} iterations'.format(it))\n",
    "        \n",
    "    if verbose == 1 or verbose == 2:\n",
    "        print('The final point is ({:.3}, {:.3})'.format(x[0],x[1]))\n",
    "            \n",
    "        if verbose == 2:\n",
    "            print('The optimal value function is {:.3} \\n'.format(ff))\n",
    "            \n",
    "    return x, ff, it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The algorithm converges in 7 iterations\n",
      "The final point is (0.587, -0.0347)\n",
      "The optimal value function is -0.363 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "x0=np.random.rand(2)*10\n",
    "x, ff, it = qnewton(f2, x0, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The algorithm converges in 35 iterations\n",
      "The final point is (1.0, 1.0)\n",
      "The optimal value function is 2.75e-17 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([-1.2,1.0]) # difficult initial point\n",
    "x, ff, it = qnewton(f4, x0, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
